{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch \n",
    "\n",
    "from mvdream.camera_utils import get_camera\n",
    "from mvdream.ldm.util import instantiate_from_config\n",
    "from mvdream.ldm.models.diffusion.ddim import DDIMSampler\n",
    "from mvdream.model_zoo import build_model\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    linear_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):# and name in target_modules:\n",
    "            linear_params += sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    print(f\"Linear Trainable parameters: {linear_params}\")\n",
    "    print(f\"LORA Trainable parameters: {trainable_params}\")\n",
    "    print(f\"LORA Aprrox Percentage : {100*trainable_params/linear_params:.2f}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args_dict = {'model_name': 'sd-v2.1-base-4view', 'config_path': None, 'ckpt_path': None, 'text': 'an astronaut riding a horse', 'suffix': ', 3d asset', 'size': 256, 'num_frames': 4, 'use_camera': 1, 'camera_elev': 15, 'camera_azim': 90, 'camera_azim_span': 360, 'seed': 23, 'fp16': False, 'device': 'cuda:2'}\n",
    "args = argparse.Namespace(**args_dict)\n",
    "dtype = torch.float16 if args.fp16 else torch.float32\n",
    "device = args.device\n",
    "batch_size = max(4, args.num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load t2i model ... \n",
      "Loading model from config: sd-v2-base.yaml\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Loading model from cache file: /data/gurunara/.cache/huggingface/hub/models--MVDream--MVDream/snapshots/d14ac9d78c48c266005729f2d5633f6c265da467/sd-v2.1-base-4view.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"load t2i model ... \")\n",
    "if args.config_path is None:\n",
    "    model = build_model(args.model_name, ckpt_path=args.ckpt_path)\n",
    "else:\n",
    "    assert args.ckpt_path is not None, \"ckpt_path must be specified!\"\n",
    "    config = OmegaConf.load(args.config_path)\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.load_state_dict(torch.load(args.ckpt_path, map_location=device))\n",
    "model.device = device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "import re\n",
    "model_modules = str(model.modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'proj_out', '1', 'proj_in', 'conv_out', 'conv2', '0', '3', 'conv_in', 'v', 'post_quant_conv', 'to_v', 'q', 'quant_conv', 'skip_connection', 'to_k', 'conv', 'to_q', 'op', 'nin_shortcut', 'proj', 'c_fc', 'c_proj', 'conv1', 'k']\n",
      "Total Parameters: 951226027\n",
      "Linear Layer Parameters: 303066240\n",
      "Percentage of Parameters in Linear Layers: 31.86%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "model_modules = str(model.modules)\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layer_names = re.findall(pattern, model_modules)\n",
    "pattern = r'\\((\\w+)\\): Conv2d'\n",
    "conv_layer_names = re.findall(pattern, model_modules)\n",
    "names = []\n",
    "# Print the names of the Linear layers\n",
    "for name in linear_layer_names:\n",
    "    names.append(name)\n",
    "for name in conv_layer_names:\n",
    "    names.append(name)\n",
    "target_modules = list(set(names))\n",
    "print(target_modules)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Function to get the number of parameters in a module\n",
    "def get_num_params(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate total number of parameters in the model\n",
    "total_params = get_num_params(model)\n",
    "\n",
    "# Calculate number of parameters in the target modules (Linear layers)\n",
    "linear_params = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):# and name in target_modules:\n",
    "        linear_params += get_num_params(module)\n",
    "\n",
    "# Calculate the percentage of parameters in Linear layers\n",
    "percentage_linear_params = (linear_params / total_params) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Linear Layer Parameters: {linear_params}\")\n",
    "print(f\"Percentage of Parameters in Linear Layers: {percentage_linear_params:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Trainable parameters: 11559040\n",
      "LORA Trainable parameters: 14638419\n",
      "LORA Aprrox Percentage : 126.64\n",
      "Total parameters: 1319633268\n",
      "Percentage of trainable parameters: 1.11%\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "r=20,\n",
    "lora_alpha=32,\n",
    "target_modules=['proj_out', 'to_k', 'c_proj', 'to_q', 'proj_in', 'to_v', 'c_fc', 'proj','conv_out', 'conv2','conv_in','conv'],\n",
    "lora_dropout=0.1,\n",
    "bias=\"lora_only\",\n",
    "modules_to_save=[\"decode_head\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdreamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
